(window.webpackJsonp=window.webpackJsonp||[]).push([[27],{468:function(s,t,a){"use strict";a.r(t);var n=a(2),r=Object(n.a)({},(function(){var s=this,t=s._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[t("div",{staticClass:"custom-block tip"},[t("p",{staticClass:"title"}),t("p",[t("font",{attrs:{size:"4"}},[s._v("摘要")])],1),s._v(" "),t("p",[s._v("机器学习实训课程笔记 😈 😈 😈")]),s._v(" "),t("p",[s._v("开发工具："),t("code",[s._v("annaconda")]),s._v(" + "),t("code",[s._v("jupyter notebook")])])]),t("h2",{attrs:{id:"xgboost"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#xgboost"}},[s._v("#")]),s._v(" XGBoost")]),s._v(" "),t("h3",{attrs:{id:"boost思想"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#boost思想"}},[s._v("#")]),s._v(" boost思想")]),s._v(" "),t("ul",[t("li",[t("p",[s._v("Boosting算法和Bagging算法一样,也是一种集成学习算法。")])]),s._v(" "),t("li",[t("p",[s._v("Boosting算法不是对样本进行随机抽样构造训练集，而是重点关注被前面的"),t("strong",[s._v("弱分类器")]),s._v("错分的样本。每一次迭代都根据上一次迭代的预测结果对样本进行加权，所以随着迭代不断进行，误差会越来越小。")])])]),s._v(" "),t("h3",{attrs:{id:"xgboost原理"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#xgboost原理"}},[s._v("#")]),s._v(" XGBoost原理")]),s._v(" "),t("p",[s._v("XGBoost可以说是GBDT（Gradient Boosting Decision Tree）梯度提升树的一个改进版本。XGBoost中的X代表的就是eXtreme（极致），XGBoost能够更快的、更高效率的训练模型。")]),s._v(" "),t("p",[s._v("👉"),t("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/258564378",target:"_blank",rel:"noopener noreferrer"}},[s._v("link1"),t("OutboundLink")],1),s._v(" "),t("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/562983875",target:"_blank",rel:"noopener noreferrer"}},[s._v("link2"),t("OutboundLink")],1),s._v("👈")]),s._v(" "),t("h3",{attrs:{id:"代码实战"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#代码实战"}},[s._v("#")]),s._v(" 代码实战")]),s._v(" "),t("p",[s._v("👉"),t("a",{attrs:{href:"https://xgboost.readthedocs.io/en/stable/",target:"_blank",rel:"noopener noreferrer"}},[s._v("xgboost"),t("OutboundLink")],1),s._v("👈")]),s._v(" "),t("p",[s._v("XGB提供了多种接口，既可以使用python原生的xgboost接口，也可以使用sklearn提供的接口。为了和前文相结合，这里主要采用sklearn接口。")]),s._v(" "),t("p",[s._v("还是以Titanic数据集为例：")]),s._v(" "),t("p",[s._v("🐴🐴🐴")]),s._v(" "),t("div",{staticClass:"language-py line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-py"}},[t("code",[t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#数据处理，特征工程")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("model_selection "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" train_test_split\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" numpy "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" np\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" pandas "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" pd\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" matplotlib"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("pyplot "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" plt\n\ndf_train "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" pd"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("read_csv"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"./data/Titanic_train.csv"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\ndf_test "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" pd"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("read_csv"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"./data/Titanic_test.csv"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#数据特征衍生，对姓名这一信息进一步挖掘，从姓名中提取身份特征等有效信息")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("getTitle")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("name"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    str1"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("name"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("split"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("','")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n    str2"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("str1"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("split"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'.'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n    str3"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("str2"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("strip"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#strip()用于移除字符串头尾指定的字符（默认为空格）")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" str3\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#存放提取后的特征")]),s._v("\ntitleDf"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("pd"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("DataFrame"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#map函数，对Series每个数据应用自定义的函数计算")]),s._v("\ntitleDf"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Title'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("df_train"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Name'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("map")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("getTitle"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#查看Title有多少类别")]),s._v("\ntitleDf"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("groupby"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Title'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("count"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#Title总共有18个类别，对其进行归类")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[s._v("'''\n定义以下几种头衔类别：\nOfficer政府官员\nRoyalty王室（皇室）\nMr已婚男士\nMrs已婚妇女\nMiss年轻未婚女子\nMaster有技能的人/教师\n'''")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#姓名中头衔字符串与定义头衔类别的映射关系")]),s._v("\ntitle_mapDict"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n                    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Capt"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("       "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Officer"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Col"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("        "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Officer"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Major"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("      "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Officer"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Jonkheer"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("   "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Royalty"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Don"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("        "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Royalty"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Sir"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("       "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Royalty"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Dr"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("         "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Officer"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Rev"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("        "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Officer"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"the Countess"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Royalty"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Dona"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("       "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Royalty"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Mme"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("        "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Mrs"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Mlle"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("       "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Miss"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Ms"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("         "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Mrs"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Mr"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("        "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Mr"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Mrs"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("       "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Mrs"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Miss"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("      "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Miss"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Master"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Master"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Lady"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("      "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Royalty"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#map函数，对Series每个数据应用自定义的函数计算")]),s._v("\ntitleDf"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Title'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("titleDf"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Title'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("map")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("title_mapDict"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#使用get_dummies进行one-hot编码")]),s._v("\ntitleDf"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("pd"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("get_dummies"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("titleDf"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Title'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\ntitleDf"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("head"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("     "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#与前面结果同理，暂不展示")]),s._v("\n \n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#添加one-hot编码产生的虚拟变量（dummy variables）到泰坦尼克号数据集full")]),s._v("\ndf_train"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("pd"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("concat"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("df_train"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("titleDf"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("axis"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#删掉部分列")]),s._v("\ndf_train"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("drop"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Name'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("axis"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("inplace"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\ntrain_data "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" df_train"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("drop"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'PassengerId'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Ticket'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("axis "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 数据统计（数据可视化）")]),s._v("\nds "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" train_data"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Sex"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("value_counts"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nplt"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("pie"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("ds "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("labels "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" ds"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("index"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("autopct"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'%1.1f%%'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\ndf2 "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" pd"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("crosstab"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("df_train"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Survived"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("df_train"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("SibSp"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#数据处理")]),s._v("\nage_mean "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" train_data"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Age'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("mean"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\ntrain_data"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Age'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("fillna"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("age_mean"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("inplace"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#缺省的年龄用均值进行填充")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#独热编码")]),s._v("\ndf_Embarked "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" pd"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("get_dummies"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("train_data"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Embarked"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("prefix"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Embarked'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\ndf_Pclass "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" pd"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("get_dummies"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("train_data"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Pclass"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("prefix"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Pclass'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\ndf_Sex "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" pd"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("get_dummies"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("train_data"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Sex"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("prefix"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Sex'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#拼接部分列，删除部分列")]),s._v("\ndf_ft "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" pd"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("concat"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("train_data"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("df_Embarked"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("df_Pclass"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("df_Sex"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("axis "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\ndf_ft"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("drop"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Pclass'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Sex'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Cabin'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Embarked'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("inplace"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("axis "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nft_names "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Age'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'SibSp'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Parch'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Fare'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Master'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Miss'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Mr'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n       "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Mrs'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Officer'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Royalty'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Embarked_C'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Embarked_Q'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Embarked_S'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n       "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Pclass_1'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Pclass_2'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Pclass_3'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Sex_female'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Sex_male'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\ntarget_names "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Survived'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 数据集分割")]),s._v("\ntrain_x"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("test_x"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("train_y"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("test_y "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" train_test_split"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("df_ft"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("ft_names"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("df_ft"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("target_names"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("test_size"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.3")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br"),t("span",{staticClass:"line-number"},[s._v("18")]),t("br"),t("span",{staticClass:"line-number"},[s._v("19")]),t("br"),t("span",{staticClass:"line-number"},[s._v("20")]),t("br"),t("span",{staticClass:"line-number"},[s._v("21")]),t("br"),t("span",{staticClass:"line-number"},[s._v("22")]),t("br"),t("span",{staticClass:"line-number"},[s._v("23")]),t("br"),t("span",{staticClass:"line-number"},[s._v("24")]),t("br"),t("span",{staticClass:"line-number"},[s._v("25")]),t("br"),t("span",{staticClass:"line-number"},[s._v("26")]),t("br"),t("span",{staticClass:"line-number"},[s._v("27")]),t("br"),t("span",{staticClass:"line-number"},[s._v("28")]),t("br"),t("span",{staticClass:"line-number"},[s._v("29")]),t("br"),t("span",{staticClass:"line-number"},[s._v("30")]),t("br"),t("span",{staticClass:"line-number"},[s._v("31")]),t("br"),t("span",{staticClass:"line-number"},[s._v("32")]),t("br"),t("span",{staticClass:"line-number"},[s._v("33")]),t("br"),t("span",{staticClass:"line-number"},[s._v("34")]),t("br"),t("span",{staticClass:"line-number"},[s._v("35")]),t("br"),t("span",{staticClass:"line-number"},[s._v("36")]),t("br"),t("span",{staticClass:"line-number"},[s._v("37")]),t("br"),t("span",{staticClass:"line-number"},[s._v("38")]),t("br"),t("span",{staticClass:"line-number"},[s._v("39")]),t("br"),t("span",{staticClass:"line-number"},[s._v("40")]),t("br"),t("span",{staticClass:"line-number"},[s._v("41")]),t("br"),t("span",{staticClass:"line-number"},[s._v("42")]),t("br"),t("span",{staticClass:"line-number"},[s._v("43")]),t("br"),t("span",{staticClass:"line-number"},[s._v("44")]),t("br"),t("span",{staticClass:"line-number"},[s._v("45")]),t("br"),t("span",{staticClass:"line-number"},[s._v("46")]),t("br"),t("span",{staticClass:"line-number"},[s._v("47")]),t("br"),t("span",{staticClass:"line-number"},[s._v("48")]),t("br"),t("span",{staticClass:"line-number"},[s._v("49")]),t("br"),t("span",{staticClass:"line-number"},[s._v("50")]),t("br"),t("span",{staticClass:"line-number"},[s._v("51")]),t("br"),t("span",{staticClass:"line-number"},[s._v("52")]),t("br"),t("span",{staticClass:"line-number"},[s._v("53")]),t("br"),t("span",{staticClass:"line-number"},[s._v("54")]),t("br"),t("span",{staticClass:"line-number"},[s._v("55")]),t("br"),t("span",{staticClass:"line-number"},[s._v("56")]),t("br"),t("span",{staticClass:"line-number"},[s._v("57")]),t("br"),t("span",{staticClass:"line-number"},[s._v("58")]),t("br"),t("span",{staticClass:"line-number"},[s._v("59")]),t("br"),t("span",{staticClass:"line-number"},[s._v("60")]),t("br"),t("span",{staticClass:"line-number"},[s._v("61")]),t("br"),t("span",{staticClass:"line-number"},[s._v("62")]),t("br"),t("span",{staticClass:"line-number"},[s._v("63")]),t("br"),t("span",{staticClass:"line-number"},[s._v("64")]),t("br"),t("span",{staticClass:"line-number"},[s._v("65")]),t("br"),t("span",{staticClass:"line-number"},[s._v("66")]),t("br"),t("span",{staticClass:"line-number"},[s._v("67")]),t("br"),t("span",{staticClass:"line-number"},[s._v("68")]),t("br"),t("span",{staticClass:"line-number"},[s._v("69")]),t("br"),t("span",{staticClass:"line-number"},[s._v("70")]),t("br"),t("span",{staticClass:"line-number"},[s._v("71")]),t("br"),t("span",{staticClass:"line-number"},[s._v("72")]),t("br"),t("span",{staticClass:"line-number"},[s._v("73")]),t("br"),t("span",{staticClass:"line-number"},[s._v("74")]),t("br"),t("span",{staticClass:"line-number"},[s._v("75")]),t("br"),t("span",{staticClass:"line-number"},[s._v("76")]),t("br"),t("span",{staticClass:"line-number"},[s._v("77")]),t("br"),t("span",{staticClass:"line-number"},[s._v("78")]),t("br"),t("span",{staticClass:"line-number"},[s._v("79")]),t("br"),t("span",{staticClass:"line-number"},[s._v("80")]),t("br"),t("span",{staticClass:"line-number"},[s._v("81")]),t("br"),t("span",{staticClass:"line-number"},[s._v("82")]),t("br"),t("span",{staticClass:"line-number"},[s._v("83")]),t("br"),t("span",{staticClass:"line-number"},[s._v("84")]),t("br"),t("span",{staticClass:"line-number"},[s._v("85")]),t("br"),t("span",{staticClass:"line-number"},[s._v("86")]),t("br"),t("span",{staticClass:"line-number"},[s._v("87")]),t("br"),t("span",{staticClass:"line-number"},[s._v("88")]),t("br"),t("span",{staticClass:"line-number"},[s._v("89")]),t("br"),t("span",{staticClass:"line-number"},[s._v("90")]),t("br"),t("span",{staticClass:"line-number"},[s._v("91")]),t("br")])]),t("div",{staticClass:"language-py line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-py"}},[t("code",[t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# xgboost")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" xgboost"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("sklearn "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" XGBClassifier\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("model_selection "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" GridSearchCV\n\nclf1 "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" XGBClassifier"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\nlearning_rate "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\nn_estimators"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1000")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("  "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#估计器个数（过多容易过拟合）")]),s._v("\nmax_depth"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\nmin_child_weight"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\ngamma"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\nsubsample"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.8")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\ncolsample_bytree"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.8")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\nobjective"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'binary:logistic'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\nnthread"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\nscale_pos_weight"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\nseed"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("27")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 网格调参")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 网格搜索 + 交叉验证")]),s._v("\nparams_test1 "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'n_estimators'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("list")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("range")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1000")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("100")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'max_depth'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\nother_params "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'learning_rate'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'min_child_weight'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'seed'")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("27")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'subsample'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.8")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'colsample_bytree'")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.8")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'gamma'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'reg_alpha'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'reg_lambda'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n\n\noptimized_XGB"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("GridSearchCV"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("estimator"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("clf1"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" param_grid"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("params_test1"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" scoring"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'roc_auc'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" cv"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" verbose"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" n_jobs"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\noptimized_XGB"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("fit"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("train_x"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("train_y"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nscore "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" optimized_XGB"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("cv_results_"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'mean_test_score'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\nparams "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" optimized_XGB"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("cv_results_"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'params'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" i "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("range")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("len")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("score"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'训练过程分数及参数:'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" score"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("i"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" params"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("i"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'\\n最好的分数以及参数: '")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" optimized_XGB"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("best_score_"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" optimized_XGB"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("best_params_"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br"),t("span",{staticClass:"line-number"},[s._v("18")]),t("br"),t("span",{staticClass:"line-number"},[s._v("19")]),t("br"),t("span",{staticClass:"line-number"},[s._v("20")]),t("br"),t("span",{staticClass:"line-number"},[s._v("21")]),t("br"),t("span",{staticClass:"line-number"},[s._v("22")]),t("br"),t("span",{staticClass:"line-number"},[s._v("23")]),t("br"),t("span",{staticClass:"line-number"},[s._v("24")]),t("br"),t("span",{staticClass:"line-number"},[s._v("25")]),t("br"),t("span",{staticClass:"line-number"},[s._v("26")]),t("br"),t("span",{staticClass:"line-number"},[s._v("27")]),t("br"),t("span",{staticClass:"line-number"},[s._v("28")]),t("br"),t("span",{staticClass:"line-number"},[s._v("29")]),t("br"),t("span",{staticClass:"line-number"},[s._v("30")]),t("br"),t("span",{staticClass:"line-number"},[s._v("31")]),t("br"),t("span",{staticClass:"line-number"},[s._v("32")]),t("br"),t("span",{staticClass:"line-number"},[s._v("33")]),t("br")])]),t("div",{staticClass:"language-py line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-py"}},[t("code",[s._v("y_pre "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("optimized_XGB"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v(" predict_proba"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("test_x"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("metrics "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" roc_curve\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("metrics "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" auc\nfpr"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" tpr"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" thresholds "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" roc_curve"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("test_y"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("y_pre"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nauc_score "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" auc"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("fpr"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" tpr"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nplt"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("plot"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("fpr"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("tpr"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("auc_score"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br")])])])}),[],!1,null,null,null);t.default=r.exports}}]);