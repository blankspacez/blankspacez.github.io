(window.webpackJsonp=window.webpackJsonp||[]).push([[53],{498:function(a,t,_){"use strict";_.r(t);var v=_(2),i=Object(v.a)({},(function(){var a=this,t=a._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("div",{staticClass:"custom-block tip"},[t("p",{staticClass:"title"}),t("img",{attrs:{decoding:"async",src:"/images/230701_01.png",width:"60%"}})]),t("h2",{attrs:{id:"_0-引言"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_0-引言"}},[a._v("#")]),a._v(" 0. 引言")]),a._v(" "),t("p",[a._v("行为识别：对包含人体动作行为的视频序列进行动作特征提取、特征表示和动作识别等操作的过程。应用场景包括：视频检索、人机交互、医学监测和自动驾驶等。")]),a._v(" "),t("p",[a._v("关键：特征的提取和表示。")]),a._v(" "),t("p",[a._v("方法：1.手工制作 2.网络学习。 如图所示：")]),a._v(" "),t("img",{attrs:{decoding:"async",src:"/images/230701_02.png",width:"50%"}}),a._v(" "),t("p",[a._v("从数据驱动的角度出发，可将行为识别方法分为基于RGB数据的方法、基于深度数据的方法、基于骨骼数据的方法和融合以上模态数据的方法。")]),a._v(" "),t("img",{attrs:{decoding:"async",src:"/images/230701_04.png",width:"60%"}}),a._v(" "),t("blockquote",[t("p",[a._v("模态的相关知识可以参考论文"),t("a",{attrs:{href:"https://blog.csdn.net/fengdu78/article/details/125670120?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_utm_term~default-0-125670120-blog-104111179.235%5Ev38%5Epc_relevant_sort_base2&spm=1001.2101.3001.4242.1&utm_relevant_index=3",target:"_blank",rel:"noopener noreferrer"}},[a._v("Human Action Recognition from Various Data Modalities: A Review"),t("OutboundLink")],1)])]),a._v(" "),t("p",[a._v("本文的贡献：")]),a._v(" "),t("ul",[t("li",[t("p",[a._v("本文的数据模态分类、方法分类和数据集分类一一对应，对初学者或者长期研究者都提供了一个结构清晰的介绍和对比；")])]),a._v(" "),t("li",[t("p",[a._v("其他的行为识别综述通常注重单一模态下的论述，而本文更加全面地论述了多种数据模态和数据融合的行为识别；")])]),a._v(" "),t("li",[t("p",[a._v("近年的行为识别综述只包含深度学习，缺少早期手工特征的方法，本文分析手工特征的思想优点和深度学习的优势，进而实现优势互补；")])]),a._v(" "),t("li",[t("p",[a._v("讨论了不同数据模态的优劣性和动作识别的挑战以及未来研究方向。")])])]),a._v(" "),t("h2",{attrs:{id:"_1-行为识别数据集"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-行为识别数据集"}},[a._v("#")]),a._v(" 1. 行为识别数据集")]),a._v(" "),t("p",[a._v("主流数据集："),t("br"),a._v(" "),t("img",{attrs:{decoding:"async",src:"/images/230701_03.png",width:"60%"}})]),a._v(" "),t("h2",{attrs:{id:"_2-基于rgb数据的行为识别方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-基于rgb数据的行为识别方法"}},[a._v("#")]),a._v(" 2. 基于RGB数据的行为识别方法")]),a._v(" "),t("p",[a._v("RGB数据的优点在于成本低、易获取；缺点在于对外观的变化(如光线变化)缺少鲁棒性，当识别目标与背景具有相似颜色和纹理时，仅用RGB数据很难处理这个问题。")]),a._v(" "),t("p",[a._v("基于RGB的行为特征的生成方式可分为手工制作和机器学习。")]),a._v(" "),t("h3",{attrs:{id:"_2-1-基于手工特征的方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-基于手工特征的方法"}},[a._v("#")]),a._v(" 2.1 基于手工特征的方法")]),a._v(" "),t("ul",[t("li",[a._v("基于时空体积的动作表示法")])]),a._v(" "),t("p",[a._v("利用3维的时空模板进行动作识别，关键在于匹配模板的构造和编码运动信息。")]),a._v(" "),t("p",[a._v("Bobick和Davis(2001)提出了MEI(motion-energy images)和MHI(motion-history images)分别表示动作发生的空间位置和动作发生的时间过程，MEI提取空间特征，MHI提取时间特征。")]),a._v(" "),t("p",[a._v("Klaser等人(2008)在2D HOG(histogram of oriented gradient)的基础上，拓展出3D HOG特征来描述人体行为，提高了识别准确率。")]),a._v(" "),t("ul",[t("li",[a._v("基于时空兴趣点的方法")])]),a._v(" "),t("p",[a._v("较时空体积法对背景的要求降低，它通过提取运动变化明显的关键区域来表示动作，重点在于关键兴趣点的检测方法、描述的特征和分类方法。")]),a._v(" "),t("p",[a._v("Chakraborty等人(2012)提出了一种改进后的3D-Harris方法，将局部特征检测技术从图像扩展到3维时空域，然后计算特征描述子，并利用描述行为的视觉词袋模型来构建视觉单词词汇表，用于加强对行为的描述。")]),a._v(" "),t("p",[a._v("Nguyen等人(2015)提出了一种基于时空注意机制的关键区域提取方法，将密集采样与视频显著信息驱动的时空特征池相结合，构造视觉词典和动作特征。")]),a._v(" "),t("ul",[t("li",[a._v("基于骨骼关节轨迹的方法")])]),a._v(" "),t("p",[a._v("为减少遮挡和相机视角变化的干扰，基于骨骼关节轨迹的方法从RGB图像中提取骨骼关键点或者跟踪人体骨骼运动的轨迹，根据关键点和轨迹判断动作的类别。该方法的关键在于使用何种算法和模型从RGB图像中提取关键点或者轨迹。")]),a._v(" "),t("p",[a._v("Gaidon等人(2014)基于分裂聚类法表示局部运动轨迹，计算轨迹特征并用聚类结果表示不同运动类别。")]),a._v(" "),t("p",[a._v("Wang和Schmid(2013)借鉴兴趣点密集采样的思想，通过采集密集点云和光流法跟踪特征点，获取密集轨迹(improved dense trajectories，iDT)，然后计算位移信息进行识别。")]),a._v(" "),t("h3",{attrs:{id:"_2-2-基于深度学习的方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-基于深度学习的方法"}},[a._v("#")]),a._v(" 2.2 基于深度学习的方法")]),a._v(" "),t("p",[a._v("基于深度学习网络提取的高层次特征，信息量丰富、有区分性，优于传统手工特征，应用于行为识别领域取得了重大的突破。")]),a._v(" "),t("ul",[t("li",[t("p",[a._v("在2D-CNN(convolutional neural networks)的基础上，Carreira和Zisserman(2017)提出了一种I3D(two-stream inflated 3D ConvNets)模型，将卷积从2维扩展到3维，并提出了双流3D卷积网络用于动作识别。\n并将许多经典算法在此数据集上进行实验对比，分析各算法的优缺点。")])]),a._v(" "),t("li",[t("p",[a._v("Zhu等人(2018)提出了一种名为隐式双流神经网络结构的CNN体系结构(hidden two-stream convolutional networks)，将原始视频帧作为输入并直接预测动作类别，通过隐式捕获相邻帧之间的运动信息，使用端到端的方法解决了需要计算光流的问题。")])]),a._v(" "),t("li",[t("p",[a._v("为了保证时空流之间的可分辨性和探索互补信息，Zhang等人(2019)提出了一种新颖的协同跨流网络，该网络调查多种不同模式中的联合信息，通过端到端的学习方式提取共同空间和时间流的网络特征，探索出不同流特征之间的相关性，从中提取不同模态的互补信息。")])]),a._v(" "),t("li",[t("p",[a._v("为了解决光流的计算复杂度问题，Kwon等人(2020)用运动特征的内部信息和轻量级学习代替对光流的繁重计算，提出了一种名为MotionSqueeze的可训练神经模块，用于有效的运动特征提取。")])]),a._v(" "),t("li",[t("p",[a._v("Gowda等人(2020)从帧选择的角度出发，保留行为特征在时间序列上区别明显的“好”帧，剔除特征类似和无法分类的帧，提出一种名为SMART的智能帧选择网络。")])]),a._v(" "),t("li",[t("p",[a._v("Qiu等人(2019)注意到视频是具有复杂时间变化的信息密集型媒体，而神经网络中的卷积滤波器都是局部操作，忽略了视频帧之间的相关性，提出了一种新的基于局部和全局扩散的时空表示学习框架，并行学习局部和全局表示。")])])]),a._v(" "),t("h2",{attrs:{id:"_3-基于深度数据的行为识别方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-基于深度数据的行为识别方法"}},[a._v("#")]),a._v(" 3. 基于深度数据的行为识别方法")]),a._v(" "),t("p",[a._v("RGB数据受干扰性较大，促使了深度数据的产生。深度图中的纹理和颜色信息少，将图像采集器到场景中各点的距离(深度)作为像素值，对光照的鲁棒性强。")]),a._v(" "),t("h3",{attrs:{id:"_3-1-基于运动变化和外观信息的方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-基于运动变化和外观信息的方法"}},[a._v("#")]),a._v(" 3.1 基于运动变化和外观信息的方法")]),a._v(" "),t("ul",[t("li",[t("p",[a._v("Yang等人(2012)通过深度运动图(depth motion map，DMM)来投影和压缩时空深度结构，再从正面，侧面和俯视图形成3个运动历史图。然后，利用HOG(histogram of oriented gradient)特征表示这些运动历史图，并将生成的HOG特征串联起来以描述动作。除此之外，Yang等人基于深度序列构造一个超向量特征来表示动作，通过连接来自深度视频的局部相邻超曲面法线来扩展HON4D(histogram oriented 4D normals)，联合局部形状和运动信息，引入了一种自适应时空金字塔，将深度视频细分为一组时空单元，以获得更具鉴别力的特征。")])]),a._v(" "),t("li",[t("p",[a._v("为了剔除噪声影响，Xia和Aggarwal(2013)提出了一种新的深度长方体相似性特征，用来描述具有自适应支撑尺寸的3维深度长方体，从而获得更可靠的时空兴趣点。")])]),a._v(" "),t("li",[t("p",[a._v("Chen和Guo(2015)通过分析前、侧和上方向的时空结构，提取时空兴趣点的运动轨迹形状和边界直方图特征，以及每个视图中的密集样本点和关节点来描述动作。")])])]),a._v(" "),t("h3",{attrs:{id:"_3-2-基于深度学习的方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-基于深度学习的方法"}},[a._v("#")]),a._v(" 3.2 基于深度学习的方法")]),a._v(" "),t("ul",[t("li",[t("p",[a._v("Wang等人(2018a)提出了3种简单、紧凑而有效的深度序列表示方法，分别称为动态深度图像(dynamic depth images，DDI)、动态深度法线图像(dynamic depth normal images，DDNI)和动态深度运动法线图像(dynamic depth motion normal images，DDMNI)，用于孤立和连续动作识别。")])]),a._v(" "),t("li",[t("p",[a._v("Trelinski和Kwolek(2019)提出了一种基于深度图序列的动作识别算法。")])]),a._v(" "),t("li",[t("p",[a._v("深度图和点云可以相互转换，并且点云的表示简单，有非常统一的结构，避免组合的不规则性和复杂性。因此，Wang等人(2020)提出了3维动态像素(3D dynamic voxel，3DV)作为新颖的3维运动表示。")])]),a._v(" "),t("li",[t("p",[a._v("Wang等人(2015)将卷积网络与深度图结合起来，通过卷积网络来学习深度图像序列的动作特征。利用分层深度运动映射(hierarchical depth motion maps，HDMMs)来提取人体的形状和运动信息，然后在HDMMs上训练一个卷积神经网络进行人体动作识别。在此基础上，Liu和Xu(2021)设计一个端到端的几何运动网络(GeometryMotion-Net)，分别利用点云网络提取运动特征和几何特征，而3DV PointNet不能进行端到端的训练。")])])]),a._v(" "),t("h2",{attrs:{id:"_4-基于骨骼数据的行为识别方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-基于骨骼数据的行为识别方法"}},[a._v("#")]),a._v(" 4 基于骨骼数据的行为识别方法")]),a._v(" "),t("p",[a._v("该方法通过骨骼关节实时对3D人体关节位置进行编码，实现人体行为的动作识别。")]),a._v(" "),t("h3",{attrs:{id:"_4-1-基于骨骼特征提取的方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-基于骨骼特征提取的方法"}},[a._v("#")]),a._v(" 4.1 基于骨骼特征提取的方法")]),a._v(" "),t("ul",[t("li",[t("p",[a._v("Vemulapalli等人(2014)提出了一种新的骨骼表示法，利用3维空间中的旋转和平移来模拟身体各个部位之间的3维几何关系。")])]),a._v(" "),t("li",[t("p",[a._v("Koniusz等人(2016)使用张量表示来捕捉3维人体关节之间的高阶关系，用于动作识别，该方法采用两种不同的核，称为序列相容核和动态相容核。")])])]),a._v(" "),t("h3",{attrs:{id:"_4-2-基于深度学习的方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-基于深度学习的方法"}},[a._v("#")]),a._v(" 4.2 基于深度学习的方法")]),a._v(" "),t("ul",[t("li",[t("p",[a._v("Liu等人(2016)通过对骨架序列进行树结构的遍历，获得了空间域的隐藏关系。同时使用带信任门的长短期记忆网络(long short-term memory, LSTM)对输入进行判别，通过潜在的空间信息来更新存储单元。")])]),a._v(" "),t("li",[t("p",[a._v("Caetano等人(2019)提出了一种基于运动信息的新表示，称为SkeleMotion。它通过计算骨骼关节的大小和方向值来编码形成每行的动作信息和每列的描述时间信息，形成调整后的骨骼图像。")])]),a._v(" "),t("li",[t("p",[a._v("基于图卷积的行为识别技术关键在于骨骼的表示，即如何将原始数据组织成拓扑图。Yan等人(2018)首先提出了一种新的基于骨架的动作识别模型，即时空图卷积网络(spatial-temporal graph convolutional networks，ST-GCN)。在此基础上，Li等人(2019)提出的AS-GCN(actional-structural graph convolutional networks)不仅可以识别人的动作，而且可以利用多任务学习策略输出对物体下一个可能姿势的预测。")])]),a._v(" "),t("li",[t("p",[a._v("Shi等人(2020)提出了一种新的多流注意增强自适应图卷积神经网络来进行基于骨架的动作识别。")])]),a._v(" "),t("li",[t("p",[a._v("Obinata和Yamamoto (2021)从另一角度注意到帧间的拓扑图，不仅仅在帧间同一关节对应的顶点之间进行连接，在帧间多个相邻顶点之间添加连接，并提取额外的特征，实现识别率的提高。改进拓扑图后的识别效果理想，使得后续的许多研究都着重于这一点，如设计动态可训练拓扑图(Ye等，2020)、各通道独享的拓扑图(Cheng等，2020a)以及结合全局和局部的拓扑图(Chen等，2021a)。")])])]),a._v(" "),t("h2",{attrs:{id:"_5-基于数据融合的行为识别方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-基于数据融合的行为识别方法"}},[a._v("#")]),a._v(" 5 基于数据融合的行为识别方法")]),a._v(" "),t("img",{attrs:{decoding:"async",src:"/images/230701_05.png",width:"70%"}}),a._v(" "),t("h3",{attrs:{id:"_5-1-基于rgb模态和深度模态的融合方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-1-基于rgb模态和深度模态的融合方法"}},[a._v("#")]),a._v(" 5.1 基于RGB模态和深度模态的融合方法")]),a._v(" "),t("p",[a._v("深度模态没有RGB模态的纹理和颜色信息，RGB模态比深度模态在空间上少一个深度信息的维度，因此两者的数据模态可以很好地互补对方缺失的特征信息。大量研究结果表明了此种融合方法的合理性和优越性。")]),a._v(" "),t("ul",[t("li",[t("p",[a._v("Jalal等人(2017)从连续的深度图序列中分割人体深度轮廓，并提取4个骨骼关节特征和一个体形特征形成时空多融合特征，利用多融合特征的编码向量进行模型训练。")])]),a._v(" "),t("li",[t("p",[a._v("Yu等人(2020)使用卷积神经网络分别训练多模态数据，并在适当位置进行RGB和深度特征的实时融合，通过局部混合的合成获得更具代表性的特征序列，提高了相似行为的识别性能。同时引入了一种改进的注意机制，实时分配不同的权值来分别关注每一帧。")])]),a._v(" "),t("li",[t("p",[a._v("Ren等人(2021)设计了一个分段协作的卷积网络(segment cooperative ConvNets, SC-ConvNets)来学习RGB-D模式的互补特征。")])])]),a._v(" "),t("h3",{attrs:{id:"_5-2-其他模态的融合方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-2-其他模态的融合方法"}},[a._v("#")]),a._v(" 5.2 其他模态的融合方法")]),a._v(" "),t("p",[a._v("融合的关键在于数据模态的选择和融合的时间。研究者需要思考一种模态融入另一种模态后的特征是否克服了原有模态的缺点，否则融合操作只会增加计算量。")]),a._v(" "),t("img",{attrs:{decoding:"async",src:"/images/230701_06.png",width:"70%"}}),a._v(" "),t("ul",[t("li",[t("p",[a._v("Elmadany等人(2018)使用规范相关分析(canonical crrelation analysis，CCA)来最大化从不同传感器提取的特征的相关性。此论文研究的特征包括从骨架数据中提取的角度数据、从深度视频中提取的深度运动图和从RGB视频提取的光流数据，通过学习这些特征共享的子空间，再使用平均池化来获取最终的特征描述符。")])]),a._v(" "),t("li",[t("p",[a._v("Rahmani等人(2014)提出一种称为深度梯度直方图的描述子，结合深度图像和3维关节位置提取的4种局部特征来处理局部遮挡，分别计算深度、深度导数和关节位置差的直方图，将每个关节运动量的变化并入全局特征向量中，形成时空特征，并使用两个随机决策森林，一个用于特征修剪，另一个用于分类，提高识别的精度。")])])])])}),[],!1,null,null,null);t.default=i.exports}}]);