<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>PyTorch编程基础 | blankspacez</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="/favicon.ico">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/2.10.0/github-markdown.min.css">
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    
    <link rel="preload" href="/assets/css/0.styles.ae38ad20.css" as="style"><link rel="preload" href="/assets/js/app.78978051.js" as="script"><link rel="preload" href="/assets/js/3.0e875ac1.js" as="script"><link rel="preload" href="/assets/js/1.38fb148a.js" as="script"><link rel="preload" href="/assets/js/20.289fd8f1.js" as="script"><link rel="prefetch" href="/assets/js/10.14f02614.js"><link rel="prefetch" href="/assets/js/11.cf747ae3.js"><link rel="prefetch" href="/assets/js/12.a0d10843.js"><link rel="prefetch" href="/assets/js/13.a0eb9684.js"><link rel="prefetch" href="/assets/js/14.f8502fb3.js"><link rel="prefetch" href="/assets/js/15.17b61f6b.js"><link rel="prefetch" href="/assets/js/16.6faba095.js"><link rel="prefetch" href="/assets/js/17.b332ca28.js"><link rel="prefetch" href="/assets/js/18.9a4b06df.js"><link rel="prefetch" href="/assets/js/19.89bccb33.js"><link rel="prefetch" href="/assets/js/21.a704d9c8.js"><link rel="prefetch" href="/assets/js/22.5acff929.js"><link rel="prefetch" href="/assets/js/23.5564d922.js"><link rel="prefetch" href="/assets/js/24.64c5db5f.js"><link rel="prefetch" href="/assets/js/25.b1795f7a.js"><link rel="prefetch" href="/assets/js/26.db30f879.js"><link rel="prefetch" href="/assets/js/27.8902b004.js"><link rel="prefetch" href="/assets/js/28.e9d2d7aa.js"><link rel="prefetch" href="/assets/js/29.b1e3d2dd.js"><link rel="prefetch" href="/assets/js/30.f40d5b99.js"><link rel="prefetch" href="/assets/js/31.0711dadf.js"><link rel="prefetch" href="/assets/js/32.f0ca0550.js"><link rel="prefetch" href="/assets/js/33.de78f25e.js"><link rel="prefetch" href="/assets/js/34.a7862a51.js"><link rel="prefetch" href="/assets/js/35.507d0088.js"><link rel="prefetch" href="/assets/js/36.c5fa8fd8.js"><link rel="prefetch" href="/assets/js/37.a8948928.js"><link rel="prefetch" href="/assets/js/38.6597238a.js"><link rel="prefetch" href="/assets/js/39.6b34d3d2.js"><link rel="prefetch" href="/assets/js/4.ce53a41d.js"><link rel="prefetch" href="/assets/js/40.f2a6036a.js"><link rel="prefetch" href="/assets/js/41.78c68a9b.js"><link rel="prefetch" href="/assets/js/42.732b7c85.js"><link rel="prefetch" href="/assets/js/43.b5975270.js"><link rel="prefetch" href="/assets/js/44.4c7ef947.js"><link rel="prefetch" href="/assets/js/45.1535ff6d.js"><link rel="prefetch" href="/assets/js/46.b4e5f14a.js"><link rel="prefetch" href="/assets/js/47.6059d462.js"><link rel="prefetch" href="/assets/js/48.a867bbe4.js"><link rel="prefetch" href="/assets/js/49.1f6e2eef.js"><link rel="prefetch" href="/assets/js/5.b7069c5a.js"><link rel="prefetch" href="/assets/js/50.af086356.js"><link rel="prefetch" href="/assets/js/51.ed979c79.js"><link rel="prefetch" href="/assets/js/52.e3bf0e65.js"><link rel="prefetch" href="/assets/js/53.beda9239.js"><link rel="prefetch" href="/assets/js/54.49f74d22.js"><link rel="prefetch" href="/assets/js/55.615460b6.js"><link rel="prefetch" href="/assets/js/56.a0fd7147.js"><link rel="prefetch" href="/assets/js/57.35daff2f.js"><link rel="prefetch" href="/assets/js/58.2d8e8bb1.js"><link rel="prefetch" href="/assets/js/59.9f6d7938.js"><link rel="prefetch" href="/assets/js/6.5fbe1b5a.js"><link rel="prefetch" href="/assets/js/60.fad337d1.js"><link rel="prefetch" href="/assets/js/61.bd015caa.js"><link rel="prefetch" href="/assets/js/62.df53e66a.js"><link rel="prefetch" href="/assets/js/63.f5bb856d.js"><link rel="prefetch" href="/assets/js/64.e6b61a85.js"><link rel="prefetch" href="/assets/js/65.b16f1d02.js"><link rel="prefetch" href="/assets/js/66.7b8566b5.js"><link rel="prefetch" href="/assets/js/67.2ee5eef8.js"><link rel="prefetch" href="/assets/js/7.4ce6e86d.js"><link rel="prefetch" href="/assets/js/8.84fd487c.js"><link rel="prefetch" href="/assets/js/9.3c302d64.js">
    <link rel="stylesheet" href="/assets/css/0.styles.ae38ad20.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-5bb33761><div data-v-5bb33761><div class="password-shadow password-wrapper-out" style="display:none;" data-v-59e6cb88 data-v-5bb33761 data-v-5bb33761><h3 class="title" data-v-59e6cb88>blankspacez</h3> <p class="description" data-v-59e6cb88></p> <label id="box" class="inputBox" data-v-59e6cb88><input type="password" value="" data-v-59e6cb88> <span data-v-59e6cb88>Konck! Knock!</span> <button data-v-59e6cb88>OK</button></label> <div class="footer" data-v-59e6cb88><span data-v-59e6cb88><i class="iconfont reco-theme" data-v-59e6cb88></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-59e6cb88>vuePress-theme-reco</a></span> <span data-v-59e6cb88><i class="iconfont reco-copyright" data-v-59e6cb88></i> <a data-v-59e6cb88><span data-v-59e6cb88>blankspacez</span>
          
        <span data-v-59e6cb88>2019 - </span>
        2025
      </a></span></div></div> <div class="hide" data-v-5bb33761><header class="navbar" data-v-5bb33761><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/logo.png" alt="blankspacez" class="logo"> <span class="site-name">blankspacez</span></a> <div class="links"><div class="color-picker"><a class="color-button"><i class="iconfont reco-color"></i></a> <div class="color-picker-menu" style="display:none;"><div class="mode-options"><h4 class="title">Choose mode</h4> <ul class="color-mode-options"><li class="dark">dark</li><li class="auto active">auto</li><li class="light">light</li></ul></div></div></div> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link"><i class="iconfont reco-home"></i>
  首页
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      分类
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/Life/" class="nav-link"><i class="undefined"></i>
  Life
</a></li><li class="dropdown-item"><!----> <a href="/categories/Algorithm/" class="nav-link"><i class="undefined"></i>
  Algorithm
</a></li><li class="dropdown-item"><!----> <a href="/categories/Study Notes/" class="nav-link"><i class="undefined"></i>
  Study Notes
</a></li><li class="dropdown-item"><!----> <a href="/categories/Tricks/" class="nav-link"><i class="undefined"></i>
  Tricks
</a></li></ul></div></div><div class="nav-item"><a href="/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  标签
</a></div><div class="nav-item"><a href="/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  时间轴
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-other"></i>
      更多
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blogs/AboutMe.html" class="nav-link"><i class="iconfont reco-account"></i>
  关于我
</a></li><li class="dropdown-item"><!----> <a href="https://github.com/blankspacez" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://blog.csdn.net/rookiezz" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-csdn"></i>
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://weibo.com/u/7194042216" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-weibo"></i>
  新浪微博
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-5bb33761></div> <aside class="sidebar" data-v-5bb33761><div class="personal-info-wrapper" data-v-1fad0c41 data-v-5bb33761><img src="/avatar.png" alt="author-avatar" class="personal-img" data-v-1fad0c41> <h3 class="name" data-v-1fad0c41>
    blankspacez
  </h3> <div class="num" data-v-1fad0c41><div data-v-1fad0c41><h3 data-v-1fad0c41>56</h3> <h6 data-v-1fad0c41>文章</h6></div> <div data-v-1fad0c41><h3 data-v-1fad0c41>35</h3> <h6 data-v-1fad0c41>标签</h6></div></div> <ul class="social-links" data-v-1fad0c41></ul> <hr data-v-1fad0c41></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link"><i class="iconfont reco-home"></i>
  首页
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      分类
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/Life/" class="nav-link"><i class="undefined"></i>
  Life
</a></li><li class="dropdown-item"><!----> <a href="/categories/Algorithm/" class="nav-link"><i class="undefined"></i>
  Algorithm
</a></li><li class="dropdown-item"><!----> <a href="/categories/Study Notes/" class="nav-link"><i class="undefined"></i>
  Study Notes
</a></li><li class="dropdown-item"><!----> <a href="/categories/Tricks/" class="nav-link"><i class="undefined"></i>
  Tricks
</a></li></ul></div></div><div class="nav-item"><a href="/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  标签
</a></div><div class="nav-item"><a href="/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  时间轴
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-other"></i>
      更多
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blogs/AboutMe.html" class="nav-link"><i class="iconfont reco-account"></i>
  关于我
</a></li><li class="dropdown-item"><!----> <a href="https://github.com/blankspacez" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://blog.csdn.net/rookiezz" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-csdn"></i>
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://weibo.com/u/7194042216" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-weibo"></i>
  新浪微博
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-59e6cb88 data-v-5bb33761><h3 class="title" data-v-59e6cb88>PyTorch编程基础</h3> <!----> <label id="box" class="inputBox" data-v-59e6cb88><input type="password" value="" data-v-59e6cb88> <span data-v-59e6cb88>Konck! Knock!</span> <button data-v-59e6cb88>OK</button></label> <div class="footer" data-v-59e6cb88><span data-v-59e6cb88><i class="iconfont reco-theme" data-v-59e6cb88></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-59e6cb88>vuePress-theme-reco</a></span> <span data-v-59e6cb88><i class="iconfont reco-copyright" data-v-59e6cb88></i> <a data-v-59e6cb88><span data-v-59e6cb88>blankspacez</span>
          
        <span data-v-59e6cb88>2019 - </span>
        2025
      </a></span></div></div> <div data-v-5bb33761><div data-v-5bb33761><main class="page"><section style="display:;"><div class="page-title"><h1 class="title">PyTorch编程基础</h1> <div data-v-8a445198><i class="iconfont reco-account" data-v-8a445198><span data-v-8a445198>blankspacez</span></i> <i class="iconfont reco-date" data-v-8a445198><span data-v-8a445198>2023/7/14</span></i> <!----> <i class="tags iconfont reco-tag" data-v-8a445198><span class="tag-item" data-v-8a445198>深度学习</span><span class="tag-item" data-v-8a445198>Python</span></i></div></div> <div class="theme-reco-content content__default"><div class="custom-block tip"><p class="title"></p><p>ref: <a href="https://www.pytorchtutorial.com/docs/" target="_blank" rel="noopener noreferrer">PyTorch中文文档<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>PyTorch 是是基于以下两个目的而打造的python科学计算框架：</p> <ul><li><p>无缝替换NumPy，利用GPU的算力来加速神经网络（cuda）</p></li> <li><p>通过自动求导机制，来让神经网络的实现变得更加容易</p></li></ul></div><div class="language-py line-numbers-mode"><pre class="language-py"><code><span class="token keyword">import</span> torch
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><h2 id="张量"><a href="#张量" class="header-anchor">#</a> 张量</h2> <p>tensor</p> <h3 id="初始化"><a href="#初始化" class="header-anchor">#</a> 初始化</h3> <ul><li>直接生成张量</li></ul> <div class="language-py line-numbers-mode"><pre class="language-py"><code>data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
x_data <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x_data<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''
tensor([[1, 2],
        [3, 4]])
'''</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><ul><li>通过numpy数组生成</li></ul> <div class="language-py line-numbers-mode"><pre class="language-py"><code><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

np_array <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
x_np <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>np_array<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x_np<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''
tensor([[1, 2],
        [3, 4]])
'''</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><ul><li>通过已有的张量来生成新的张量</li></ul> <p>继承已有张量的数据属性（结构、类型），也可以重写新类型</p> <div class="language-py line-numbers-mode"><pre class="language-py"><code>x_ones <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>x_data<span class="token punctuation">)</span>   <span class="token comment"># 保留 x_data 的属性</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Ones Tensor: \n </span><span class="token interpolation"><span class="token punctuation">{</span>x_ones<span class="token punctuation">}</span></span><span class="token string"> \n&quot;</span></span><span class="token punctuation">)</span>

x_rand <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand_like<span class="token punctuation">(</span>x_data<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>   <span class="token comment"># 重写 x_data 的数据类型</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Random Tensor: \n </span><span class="token interpolation"><span class="token punctuation">{</span>x_rand<span class="token punctuation">}</span></span><span class="token string"> \n&quot;</span></span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''
Ones Tensor: 
 tensor([[1, 1],
        [1, 1]]) 

Random Tensor: 
 tensor([[0.2622, 0.2768],
        [0.2162, 0.0212]]) 
'''</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><ul><li>通过指定数据维度来生成张量</li></ul> <p>shape是元组类型, 用来描述张量的维数, 下面3个函数通过传入shape来指定生成张量的维数。</p> <div class="language-py line-numbers-mode"><pre class="language-py"><code>shape <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>
rand_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>shape<span class="token punctuation">)</span>
ones_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>shape<span class="token punctuation">)</span>
zeros_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Random Tensor: \n </span><span class="token interpolation"><span class="token punctuation">{</span>rand_tensor<span class="token punctuation">}</span></span><span class="token string"> \n&quot;</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Ones Tensor: \n </span><span class="token interpolation"><span class="token punctuation">{</span>ones_tensor<span class="token punctuation">}</span></span><span class="token string"> \n&quot;</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Zeros Tensor: \n </span><span class="token interpolation"><span class="token punctuation">{</span>zeros_tensor<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''
Random Tensor: 
 tensor([[0.7244, 0.3744, 0.9196],
        [0.5778, 0.4171, 0.0868]]) 

Ones Tensor: 
 tensor([[1., 1., 1.],
        [1., 1., 1.]]) 

Zeros Tensor: 
 tensor([[0., 0., 0.],
        [0., 0., 0.]])
'''</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br></div></div><h3 id="属性"><a href="#属性" class="header-anchor">#</a> 属性</h3> <ul><li>维数：shape</li> <li>数据类型：dtype</li> <li>所存储的设备(CPU或GPU)：device</li></ul> <h3 id="运算"><a href="#运算" class="header-anchor">#</a> 运算</h3> <div class="language-py line-numbers-mode"><pre class="language-py"><code><span class="token comment"># 判断当前环境GPU是否可用, 然后将tensor导入GPU内运行</span>
<span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    tensor <span class="token operator">=</span> tensor<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">'cuda'</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><h3 id="索引和切片"><a href="#索引和切片" class="header-anchor">#</a> 索引和切片</h3> <div class="language-py line-numbers-mode"><pre class="language-py"><code>tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>            <span class="token comment"># 将第1列(从0开始)的数据全部赋值为0</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''
tensor([[1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.]], dtype=torch.int32)
'''</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><h3 id="拼接"><a href="#拼接" class="header-anchor">#</a> 拼接</h3> <p>torch.cat 或 torch.stack，dim为拼接方向的维度</p> <div class="language-py line-numbers-mode"><pre class="language-py"><code>t1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>tensor<span class="token punctuation">,</span> tensor<span class="token punctuation">,</span> tensor<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># 0:行 1：列</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>t1<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''
tensor([[1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1],
        [1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1],
        [1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1],
        [1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1]], dtype=torch.int32)
'''</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><h3 id="张量乘积和矩阵乘法"><a href="#张量乘积和矩阵乘法" class="header-anchor">#</a> 张量乘积和矩阵乘法</h3> <div class="language-py line-numbers-mode"><pre class="language-py"><code><span class="token comment"># 逐个相乘，二者等价</span>
tensor<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>
tensor <span class="token operator">*</span> tensor

<span class="token comment"># 矩阵乘法，二者等价</span>
tensor<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>tensor<span class="token punctuation">.</span>T<span class="token punctuation">)</span>
tensor @ tensor<span class="token punctuation">.</span>T
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><h3 id="自动赋值运算"><a href="#自动赋值运算" class="header-anchor">#</a> 自动赋值运算</h3> <p>自动赋值运算通常在方法后有 _ 作为后缀，例如: x.copy_(y), x.t_()操作会改变 x 的取值。</p> <div class="language-py line-numbers-mode"><pre class="language-py"><code>tensor<span class="token punctuation">.</span>add_<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''
tensor([[6, 5, 6, 6],
        [6, 5, 6, 6],
        [6, 5, 6, 6],
        [6, 5, 6, 6]], dtype=torch.int32)
'''</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><blockquote><p>自动赋值运算虽然可以节省内存, 但在求导时会因为丢失了中间过程而导致一些问题, 所以我们并不鼓励使用它。</p></blockquote> <h2 id="autograd"><a href="#autograd" class="header-anchor">#</a> Autograd</h2> <p><code>torch.autograd</code>是 PyTorch 的自动差分引擎，可为神经网络训练提供支持。</p> <h3 id="ex-一个训练步骤示例"><a href="#ex-一个训练步骤示例" class="header-anchor">#</a> ex：一个训练步骤示例</h3> <p>从<code>torchvision</code>加载了经过预训练的<code>resnet18</code>模型。 我们创建一个随机数据张量来表示具有 3 个通道的单个图像，高度&amp;宽度为 64，其对应的label初始化为一些随机值。</p> <div class="language-py line-numbers-mode"><pre class="language-py"><code><span class="token keyword">import</span> torch<span class="token punctuation">,</span> torchvision
model <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
data <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span>
labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>正向传播:</p> <div class="language-py line-numbers-mode"><pre class="language-py"><code>prediction <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">)</span> <span class="token comment"># forward pass</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>反向传播，<code>Autograd</code> 会为每个模型参数计算梯度并将其存储在参数的<code>.grad</code>属性中。</p> <div class="language-py line-numbers-mode"><pre class="language-py"><code>loss <span class="token operator">=</span> <span class="token punctuation">(</span>prediction <span class="token operator">-</span> labels<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># backward pass</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>接下来，我们加载一个优化器，在本例中为随机梯度下降，学习率为 0.01，动量为 0.9。 我们在优化器中注册模型的所有参数。</p> <div class="language-py line-numbers-mode"><pre class="language-py"><code>optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e-2</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>调用<code>.step()</code>启动梯度下降。 优化器通过<code>.grad</code>中存储的梯度来调整每个参数。</p> <div class="language-py line-numbers-mode"><pre class="language-py"><code>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># gradient descent</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><h3 id="ex-autograd如何收集梯度"><a href="#ex-autograd如何收集梯度" class="header-anchor">#</a> ex：Autograd如何收集梯度</h3> <p>首先用<code>requires_grad=True</code>创建两个张量<code>a</code>和<code>b</code>。 这向autograd发出信号，应跟踪对它们的所有操作。</p> <div class="language-py line-numbers-mode"><pre class="language-py"><code>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">3.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">6.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>从a和b创建另一个张量<code>Q</code>。假设<code>a</code>和<code>b</code>是参数，<code>Q</code>是误差，在<code>Q</code>上调用<code>.backward()</code>时，<code>Autograd</code> 将计算这些梯度并将其存储在各个张量的<code>.grad</code>属性中。</p> <p>我们需要在<code>Q.backward()</code>中显式传递<code>gradient</code>参数，因为它是向量。 <code>gradient</code>是与<code>Q</code>形状相同的张量，它表示Q相对于本身的梯度，即
𝟙
；同样，我们也可以将<code>Q</code>聚合为一个标量，然后隐式地向后调用，例如<code>Q.sum().backward()</code>。</p> <div class="language-py line-numbers-mode"><pre class="language-py"><code>Q <span class="token operator">=</span> <span class="token number">3</span><span class="token operator">*</span>a<span class="token operator">**</span><span class="token number">3</span> <span class="token operator">-</span> b<span class="token operator">**</span><span class="token number">2</span> <span class="token comment"># 3a^3-b^2</span>
external_grad <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Q<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>gradient<span class="token operator">=</span>external_grad<span class="token punctuation">)</span> <span class="token comment"># Q是张量</span>

<span class="token comment"># 结果都是true</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token number">9</span><span class="token operator">*</span>a<span class="token operator">**</span><span class="token number">2</span> <span class="token operator">==</span> a<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token operator">*</span>b <span class="token operator">==</span> b<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><h3 id="计算图"><a href="#计算图" class="header-anchor">#</a> 计算图</h3> <p><code>Autograd</code> 在由函数对象组成的有向无环图（<code>DAG</code>）中记录数据（张量）和所有已执行的操作（以及由此产生的新张量）。 在此<code>DAG</code>中，叶子是输入张量，根是输出张量。 通过从根到叶跟踪此图，可以使用链式规则自动计算梯度。</p> <p>在正向传播中，<code>Autograd</code> 同时执行两项操作：</p> <ul><li>运行请求的操作以计算结果张量</li> <li>在 DAG 中维护操作的梯度函数</li></ul> <p>当在<code>DAG</code>根目录上调用<code>.backward()</code>时，反向传递开始。<code>Autograd</code>执行：</p> <p>从每个<code>.grad_fn</code>计算梯度，将它们累积在各自的张量的<code>.grad</code>属性中
使用链式规则一直传播到叶子张量
<code>DAG</code> 在 <code>PyTorch</code> 中是动态的。要注意的重要一点是，图是从头开始重新创建的； 在每个<code>.backward()</code>调用之后，<code>Autograd</code> 开始填充新图。 可以根据需要在每次迭代中更改形状，大小和操作。</p> <h3 id="从dag中排除"><a href="#从dag中排除" class="header-anchor">#</a> 从DAG中排除</h3> <p>对于不需要梯度的张量，将<code>requires_grad</code>属性设置为<code>False</code>会将其从梯度计算 <code>DAG</code> 中排除。但注意，即使只有一个输入张量该属性为<code>True</code>，操作的输出张量也将需要梯度。</p> <p>在<code>NN</code>中，不计算梯度的参数通常称为冻结参数。 如果事先知道您不需要这些参数的梯度，则“冻结”模型的一部分会带来性能优势。常见用例：调整预训练网络(finetuning)</p> <p>在微调中，我们冻结了大部分模型，通常仅修改分类器层以对新标签进行预测。 ex：加载一个预训练的 resnet18 模型，并冻结所有参数。</p> <div class="language-py line-numbers-mode"><pre class="language-py"><code><span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token punctuation">,</span> optim

model <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># Freeze all the parameters in the network</span>
<span class="token keyword">for</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    param<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span>

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>假设我们要在具有 10 个标签的新数据集中微调模型。 在<code>resnet</code> 中，分类器是最后一个线性层<code>model.fc</code>。 我们可以简单地将其替换为充当我们的分类器的新线性层（默认情况下未冻结）。</p> <div class="language-py line-numbers-mode"><pre class="language-py"><code>model<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>现在，除了<code>model.fc</code>的参数外，模型中的所有参数都将冻结。 计算梯度的唯一参数是<code>model.fc</code>的权重和偏差。</p> <div class="language-py line-numbers-mode"><pre class="language-py"><code><span class="token comment"># Optimize only the classifier</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>fc<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e-2</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>尽管我们在优化器中注册了所有参数，但唯一可计算梯度的参数（因此会在梯度下降中进行更新）是分类器的权重和偏差。</p> <p><code>torch.no_grad()</code>中的上下文管理器可以使用相同的排除功能。</p> <h2 id="神经网络"><a href="#神经网络" class="header-anchor">#</a> 神经网络</h2> <p>可以使用<code>torch.nn</code>包构建神经网络。<code>nn</code>依赖于<code>autograd</code>来定义模型并对其进行微分。</p> <ul><li><p>定义网络时，需要继承<code>nn.Module</code>，并实现它的<code>forward</code>方法，把网络中具有可学习参数的层放在构造函数<code>__init__</code>中。</p></li> <li><p>只要在<code>nn.Module</code>的子类中定义了<code>forward</code>函数，<code>backward</code>函数就会自动被实现(利用<code>autograd</code>)。</p></li></ul> <p>神经网络的典型训练过程：</p> <ul><li>定义具有一些可学习参数（或权重）的神经网络</li> <li>遍历输入数据集</li> <li>通过网络处理输入</li> <li>计算损失（输出正确的距离有多远）</li> <li>将梯度传播回网络参数</li> <li>通常使用简单的更新规则来更新网络的权重：<code>weight</code> = <code>weight</code> - <code>learning_rate</code> * <code>gradient</code></li></ul> <h3 id="创建网络"><a href="#创建网络" class="header-anchor">#</a> 创建网络</h3> <p>ex: 利用MNIST数据集识别手写数字</p> <div class="language-py line-numbers-mode"><pre class="language-py"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F


<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 1 input image channel, 6 output channels, 5x5 square convolution kernel</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
        <span class="token comment"># an affine operation: y = Wx + b</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span>  <span class="token comment"># 5*5 from image dimension</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>

    <span class="token comment"># 指定网络的运行过程</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Max pooling over a (2, 2) window</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># If the size is a square, you can specify with a single number</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># flatten all dimensions except the batch dimension 拉平x</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x


net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''
Net(
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
'''</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br></div></div><p>模型的可学习参数由net.parameters()返回。</p> <div class="language-py line-numbers-mode"><pre class="language-py"><code>params <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>params<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># foward函数包含10个操作</span>

<span class="token triple-quoted-string string">'''
10
'''</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>尝试一个 32*32 随机输入。</p> <div class="language-py line-numbers-mode"><pre class="language-py"><code><span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>
out <span class="token operator">=</span> net<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''
tensor([[ 0.0818, -0.0857,  0.0695,  0.1430,  0.0191, -0.1402,  0.0499, -0.0737,
         -0.0857,  0.1395]], grad_fn=&lt;AddmmBackward0&gt;)
'''</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>使用随机梯度将所有参数和反向传播的梯度缓冲区归零：</p> <div class="language-py line-numbers-mode"><pre class="language-py"><code>net<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><blockquote><p>注意：<code>torch.nn</code>仅支持<code>mini-batch</code>。 整个<code>torch.nn</code>包仅支持作为微型样本而不是单个样本的输入：ex：<code>nn.Conv2d</code>将采用<code>nSamples</code> x <code>nChannels</code> x <code>Height</code> x <code>Width</code>的 4D 张量。如果您只有一个样本，只需使用<code>input.unsqueeze(0)</code>添加一个假批量尺寸。</p></blockquote> <p>总结：</p> <ul><li><code>torch.Tensor</code>: 一个多维数组，支持诸如<code>backward()</code>的自动微分操作。 同样，保持相对于张量的梯度。</li> <li><code>nn.Module</code>: 神经网络模块。 封装参数的便捷方法，并带有将其移动到 <code>GPU</code>，导出，加载等的帮助器。</li> <li><code>nn.Parameter</code>: 一种张量，即将其分配为<code>Module</code>的属性时，自动注册为参数。</li> <li><code>autograd.Function</code>: 实现自动微分操作的正向和反向定义。 每个<code>Tensor</code>操作都会创建至少一个<code>Function</code>节点，该节点连接到创建<code>Tensor</code>的函数，并且编码其历史记录。</li></ul> <h3 id="损失函数"><a href="#损失函数" class="header-anchor">#</a> 损失函数</h3> <p>损失函数采用一对（输出，目标）输入，并计算一个值，该值估计输出与目标之间的距离。</p> <p><code>nn</code>包下有几种不同的损失函数，一个简单的损失是：<code>nn.MSELoss</code>，它计算输入和目标之间的均方误差，类似地，还有交叉熵损失<code>nn.CrossEntropyLoss</code>，ex：</p> <div class="language-py line-numbers-mode"><pre class="language-py"><code>output <span class="token operator">=</span> net<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
target <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span>  <span class="token comment"># a dummy target, for example</span>
target <span class="token operator">=</span> target<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># make it the same shape as output</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>

loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''
tensor(0.3117, grad_fn=&lt;MseLossBackward0&gt;)
'''</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>现在，如果使用.grad_fn属性向后跟随loss，您将看到一个计算图，如下所示：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d
      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear
      -&gt; MSELoss
      -&gt; loss
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>当我们调用<code>loss.backward()</code>时，整个图将被微分。 并且图中具有<code>requires_grad=True</code>的所有张量将随梯度累积其<code>.grad</code>张量。</p> <p>为了说明，让我们向后走几步：</p> <div class="language-py line-numbers-mode"><pre class="language-py"><code><span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">.</span>grad_fn<span class="token punctuation">)</span>  <span class="token comment"># MSELoss</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">.</span>grad_fn<span class="token punctuation">.</span>next_functions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># Linear</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">.</span>grad_fn<span class="token punctuation">.</span>next_functions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>next_functions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># ReLU</span>

<span class="token triple-quoted-string string">'''
&lt;MseLossBackward0 object at 0x7f71283dd048&gt;
&lt;AddmmBackward0 object at 0x7f71283dd7f0&gt;
&lt;AccumulateGrad object at 0x7f71283dd7f0&gt;
'''</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><h3 id="反向传播"><a href="#反向传播" class="header-anchor">#</a> 反向传播</h3> <p>要反向传播误差，我们要做的只是<code>loss.backward()</code>。 不过，需要清除现有的梯度，否则梯度将累积到现有的梯度中。</p> <p>ex：看一下<code>backward</code>前后<code>conv1</code>的偏差梯度</p> <div class="language-py line-numbers-mode"><pre class="language-py"><code>net<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>     <span class="token comment"># zeroes the gradient buffers of all parameters</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'conv1.bias.grad before backward'</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>conv1<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>

loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'conv1.bias.grad after backward'</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>conv1<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''
conv1.bias.grad before backward
None
conv1.bias.grad after backward
tensor([ 0.0125, -0.0111,  0.0211, -0.0378, -0.0042, -0.0194])
'''</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><h3 id="更新权重"><a href="#更新权重" class="header-anchor">#</a> 更新权重</h3> <p>实践中使用的最简单的更新规则是随机梯度下降（<code>SGD</code>）：</p> <p><code>weight</code> = <code>weight</code> - <code>learning_rate</code> * <code>gradient</code></p> <p>使用神经网络时，您希望使用各种不同的更新规则，例如 <code>SGD</code>，<code>Nesterov-SGD</code>，<code>Adam</code>，<code>RMSProp</code> 等。为实现此目的，<code>torch.optim</code>可实现所有这些方法。 使用它非常简单：</p> <div class="language-py line-numbers-mode"><pre class="language-py"><code><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim

<span class="token comment"># create your optimizer</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>

<span class="token comment"># in your training loop:</span>
optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>   <span class="token comment"># zero the gradient buffers</span>
output <span class="token operator">=</span> net<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment"># Does the update</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>注意：需要使用<code>optimizer.zero_grad()</code>将梯度缓冲区手动设置为零。 这是因为如反向传播部分中所述累积了梯度。</p></div></section> <footer class="page-edit"><!----> <!----></footer> <!----> <div class="comments-wrapper"><!----></div> <ul class="side-bar sub-sidebar-wrapper" style="width:12rem;" data-v-ac050c62><li class="level-2" data-v-ac050c62><a href="/blogs/Study%20Notes/deep%20learning/230714.html#张量" class="sidebar-link reco-side-张量" data-v-ac050c62>张量</a></li><li class="level-3" data-v-ac050c62><a href="/blogs/Study%20Notes/deep%20learning/230714.html#初始化" class="sidebar-link reco-side-初始化" data-v-ac050c62>初始化</a></li><li class="level-3" data-v-ac050c62><a href="/blogs/Study%20Notes/deep%20learning/230714.html#属性" class="sidebar-link reco-side-属性" data-v-ac050c62>属性</a></li><li class="level-3" data-v-ac050c62><a href="/blogs/Study%20Notes/deep%20learning/230714.html#运算" class="sidebar-link reco-side-运算" data-v-ac050c62>运算</a></li><li class="level-3" data-v-ac050c62><a href="/blogs/Study%20Notes/deep%20learning/230714.html#索引和切片" class="sidebar-link reco-side-索引和切片" data-v-ac050c62>索引和切片</a></li><li class="level-3" data-v-ac050c62><a href="/blogs/Study%20Notes/deep%20learning/230714.html#拼接" class="sidebar-link reco-side-拼接" data-v-ac050c62>拼接</a></li><li class="level-3" data-v-ac050c62><a href="/blogs/Study%20Notes/deep%20learning/230714.html#张量乘积和矩阵乘法" class="sidebar-link reco-side-张量乘积和矩阵乘法" data-v-ac050c62>张量乘积和矩阵乘法</a></li><li class="level-3" data-v-ac050c62><a href="/blogs/Study%20Notes/deep%20learning/230714.html#自动赋值运算" class="sidebar-link reco-side-自动赋值运算" data-v-ac050c62>自动赋值运算</a></li><li class="level-2" data-v-ac050c62><a href="/blogs/Study%20Notes/deep%20learning/230714.html#autograd" class="sidebar-link reco-side-autograd" data-v-ac050c62>Autograd</a></li><li class="level-3" data-v-ac050c62><a href="/blogs/Study%20Notes/deep%20learning/230714.html#ex-一个训练步骤示例" class="sidebar-link reco-side-ex-一个训练步骤示例" data-v-ac050c62>ex：一个训练步骤示例</a></li><li class="level-3" data-v-ac050c62><a href="/blogs/Study%20Notes/deep%20learning/230714.html#ex-autograd如何收集梯度" class="sidebar-link reco-side-ex-autograd如何收集梯度" data-v-ac050c62>ex：Autograd如何收集梯度</a></li><li class="level-3" data-v-ac050c62><a href="/blogs/Study%20Notes/deep%20learning/230714.html#计算图" class="sidebar-link reco-side-计算图" data-v-ac050c62>计算图</a></li><li class="level-3" data-v-ac050c62><a href="/blogs/Study%20Notes/deep%20learning/230714.html#从dag中排除" class="sidebar-link reco-side-从dag中排除" data-v-ac050c62>从DAG中排除</a></li><li class="level-2" data-v-ac050c62><a href="/blogs/Study%20Notes/deep%20learning/230714.html#神经网络" class="sidebar-link reco-side-神经网络" data-v-ac050c62>神经网络</a></li><li class="level-3" data-v-ac050c62><a href="/blogs/Study%20Notes/deep%20learning/230714.html#创建网络" class="sidebar-link reco-side-创建网络" data-v-ac050c62>创建网络</a></li><li class="level-3" data-v-ac050c62><a href="/blogs/Study%20Notes/deep%20learning/230714.html#损失函数" class="sidebar-link reco-side-损失函数" data-v-ac050c62>损失函数</a></li><li class="level-3" data-v-ac050c62><a href="/blogs/Study%20Notes/deep%20learning/230714.html#反向传播" class="sidebar-link reco-side-反向传播" data-v-ac050c62>反向传播</a></li><li class="level-3" data-v-ac050c62><a href="/blogs/Study%20Notes/deep%20learning/230714.html#更新权重" class="sidebar-link reco-side-更新权重" data-v-ac050c62>更新权重</a></li></ul></main></div> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div><div class="kanbanniang" data-v-5775ee02><div class="banniang-container" style="display:;" data-v-5775ee02><div class="messageBox" style="right:88px;bottom:200px;display:none;" data-v-5775ee02>
      欢迎来到我的博客
    </div> <div class="operation" style="right:220px;bottom:40px;display:;" data-v-5775ee02><i class="kbnfont kbn-ban-home ban-home" data-v-5775ee02></i> <i class="kbnfont kbn-ban-message message" data-v-5775ee02></i> <i class="kbnfont kbn-ban-close close" data-v-5775ee02></i> <a target="_blank" href="https://vuepress-theme-reco.recoluan.com/views/plugins/kanbanniang.html" data-v-5775ee02><i class="kbnfont kbn-ban-info info" data-v-5775ee02></i></a> <i class="kbnfont kbn-ban-theme skin" style="display:;" data-v-5775ee02></i></div> <canvas id="banniang" width="150" height="220" class="live2d" style="right:220px;bottom:-20px;opacity:0.9;" data-v-5775ee02></canvas></div> <div class="showBanNiang" style="display:none;" data-v-5775ee02>
    看板娘
  </div></div><canvas id="vuepress-canvas-cursor"></canvas></div></div>
    <script src="/assets/js/app.78978051.js" defer></script><script src="/assets/js/3.0e875ac1.js" defer></script><script src="/assets/js/1.38fb148a.js" defer></script><script src="/assets/js/20.289fd8f1.js" defer></script>
  </body>
</html>
